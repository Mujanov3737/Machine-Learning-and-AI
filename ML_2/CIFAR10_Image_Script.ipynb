{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2s 0us/step\n",
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 4,200,842\n",
      "Trainable params: 4,200,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 70s 2ms/step - loss: 1.7009 - accuracy: 0.3975 - val_loss: 1.3997 - val_accuracy: 0.5083\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 72s 2ms/step - loss: 1.3512 - accuracy: 0.5214 - val_loss: 1.2343 - val_accuracy: 0.5800\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 74s 2ms/step - loss: 1.2183 - accuracy: 0.5702 - val_loss: 1.2135 - val_accuracy: 0.5754\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 71s 2ms/step - loss: 1.1308 - accuracy: 0.6027 - val_loss: 1.1341 - val_accuracy: 0.6008\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 1.0579 - accuracy: 0.6292 - val_loss: 1.2333 - val_accuracy: 0.5667\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 63s 2ms/step - loss: 1.0036 - accuracy: 0.6498 - val_loss: 1.0737 - val_accuracy: 0.6308\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.9481 - accuracy: 0.6664 - val_loss: 1.1458 - val_accuracy: 0.6124\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.9075 - accuracy: 0.6825 - val_loss: 1.0336 - val_accuracy: 0.6471\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.8657 - accuracy: 0.6978 - val_loss: 1.0025 - val_accuracy: 0.6589\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.8260 - accuracy: 0.7125 - val_loss: 0.9831 - val_accuracy: 0.6665\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 63s 2ms/step - loss: 0.7925 - accuracy: 0.7249 - val_loss: 0.9662 - val_accuracy: 0.6756\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.7594 - accuracy: 0.7368 - val_loss: 0.9784 - val_accuracy: 0.6787\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.7346 - accuracy: 0.7472 - val_loss: 1.0421 - val_accuracy: 0.6654\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.7012 - accuracy: 0.7563 - val_loss: 1.0297 - val_accuracy: 0.6700\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.6779 - accuracy: 0.7637 - val_loss: 1.0099 - val_accuracy: 0.6828\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.6516 - accuracy: 0.7775 - val_loss: 1.0439 - val_accuracy: 0.6612\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.6278 - accuracy: 0.7834 - val_loss: 1.0012 - val_accuracy: 0.6836\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 72s 2ms/step - loss: 0.6183 - accuracy: 0.7877 - val_loss: 1.2098 - val_accuracy: 0.6402\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 78s 2ms/step - loss: 0.5897 - accuracy: 0.7986 - val_loss: 1.0521 - val_accuracy: 0.6808\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 68s 2ms/step - loss: 0.5736 - accuracy: 0.8028 - val_loss: 1.0629 - val_accuracy: 0.6837\n",
      "10000/10000 [==============================] - 7s 728us/step\n",
      "Test score: 1.0648894622802734\n",
      "Test accuracy: 0.6761000156402588\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert to categorical\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# train\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM,\n",
    "metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE,\n",
    "epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT,\n",
    "verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test,\n",
    "batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#save model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,676,842\n",
      "Trainable params: 1,676,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 1.7704 - accuracy: 0.3603 - val_loss: 1.3903 - val_accuracy: 0.4944\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 1.3271 - accuracy: 0.5277 - val_loss: 1.0966 - val_accuracy: 0.6094\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 1.1205 - accuracy: 0.6044 - val_loss: 0.9875 - val_accuracy: 0.6566\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 95s 2ms/step - loss: 0.9998 - accuracy: 0.6500 - val_loss: 0.9133 - val_accuracy: 0.6756\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 0.8993 - accuracy: 0.6855 - val_loss: 0.9343 - val_accuracy: 0.6768\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 0.8239 - accuracy: 0.7100 - val_loss: 0.7664 - val_accuracy: 0.7366\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 0.7666 - accuracy: 0.7329 - val_loss: 0.7607 - val_accuracy: 0.7356\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 0.7080 - accuracy: 0.7542 - val_loss: 0.8577 - val_accuracy: 0.7153\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 93s 2ms/step - loss: 0.6739 - accuracy: 0.7655 - val_loss: 0.7263 - val_accuracy: 0.7599\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 0.6354 - accuracy: 0.7815 - val_loss: 0.7149 - val_accuracy: 0.7578\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 93s 2ms/step - loss: 0.6070 - accuracy: 0.7902 - val_loss: 0.8006 - val_accuracy: 0.7397\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 0.5849 - accuracy: 0.7997 - val_loss: 0.6975 - val_accuracy: 0.7623\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 0.5664 - accuracy: 0.8033 - val_loss: 0.7542 - val_accuracy: 0.7704\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 0.5526 - accuracy: 0.8108 - val_loss: 0.6900 - val_accuracy: 0.7667\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 0.5449 - accuracy: 0.8145 - val_loss: 0.7257 - val_accuracy: 0.7765\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 95s 2ms/step - loss: 0.5352 - accuracy: 0.8173 - val_loss: 0.8064 - val_accuracy: 0.7797\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 94s 2ms/step - loss: 0.5347 - accuracy: 0.8201 - val_loss: 0.6866 - val_accuracy: 0.7764\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 93s 2ms/step - loss: 0.5366 - accuracy: 0.8203 - val_loss: 0.7701 - val_accuracy: 0.7767\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 104s 3ms/step - loss: 0.5290 - accuracy: 0.8212 - val_loss: 0.8325 - val_accuracy: 0.7604\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 105s 3ms/step - loss: 0.5293 - accuracy: 0.8221 - val_loss: 0.7222 - val_accuracy: 0.7864\n",
      "10000/10000 [==============================] - 12s 1ms/step\n",
      "Test score: 0.7675681403160095\n",
      "Test accuracy: 0.7771000266075134\n"
     ]
    }
   ],
   "source": [
    "# Define a deeper network with multiple convolutions\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert to categorical\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "# Added these lines for the deeper network\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "####\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# train\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM,\n",
    "metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE,\n",
    "epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT,\n",
    "verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test,\n",
    "batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#save model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Augmenting training set images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:86: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:104: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., epochs=20, verbose=1, steps_per_epoch=390)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,676,842\n",
      "Trainable params: 1,676,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "390/390 [==============================] - 109s 279ms/step - loss: 1.9711 - accuracy: 0.2734\n",
      "Epoch 2/20\n",
      "390/390 [==============================] - 108s 277ms/step - loss: 1.6729 - accuracy: 0.3925\n",
      "Epoch 3/20\n",
      "390/390 [==============================] - 109s 278ms/step - loss: 1.5379 - accuracy: 0.4458\n",
      "Epoch 4/20\n",
      "390/390 [==============================] - 109s 280ms/step - loss: 1.4456 - accuracy: 0.4812\n",
      "Epoch 5/20\n",
      "390/390 [==============================] - 109s 280ms/step - loss: 1.3817 - accuracy: 0.5046\n",
      "Epoch 6/20\n",
      "390/390 [==============================] - 115s 295ms/step - loss: 1.3297 - accuracy: 0.5242\n",
      "Epoch 7/20\n",
      "390/390 [==============================] - 120s 308ms/step - loss: 1.2885 - accuracy: 0.5404\n",
      "Epoch 8/20\n",
      "390/390 [==============================] - 109s 279ms/step - loss: 1.2581 - accuracy: 0.5549\n",
      "Epoch 9/20\n",
      "390/390 [==============================] - 109s 280ms/step - loss: 1.2333 - accuracy: 0.5639\n",
      "Epoch 10/20\n",
      "390/390 [==============================] - 110s 282ms/step - loss: 1.2155 - accuracy: 0.5685\n",
      "Epoch 11/20\n",
      "390/390 [==============================] - 111s 283ms/step - loss: 1.1968 - accuracy: 0.5793\n",
      "Epoch 12/20\n",
      "390/390 [==============================] - 111s 285ms/step - loss: 1.1856 - accuracy: 0.5835\n",
      "Epoch 13/20\n",
      "390/390 [==============================] - 110s 281ms/step - loss: 1.1820 - accuracy: 0.5855\n",
      "Epoch 14/20\n",
      "390/390 [==============================] - 110s 281ms/step - loss: 1.1740 - accuracy: 0.5874\n",
      "Epoch 15/20\n",
      "390/390 [==============================] - 110s 282ms/step - loss: 1.1705 - accuracy: 0.5925\n",
      "Epoch 16/20\n",
      "390/390 [==============================] - 109s 280ms/step - loss: 1.1659 - accuracy: 0.5914\n",
      "Epoch 17/20\n",
      "390/390 [==============================] - 110s 281ms/step - loss: 1.1544 - accuracy: 0.6009\n",
      "Epoch 18/20\n",
      "390/390 [==============================] - 108s 277ms/step - loss: 1.1580 - accuracy: 0.5981\n",
      "Epoch 19/20\n",
      "390/390 [==============================] - 107s 274ms/step - loss: 1.1541 - accuracy: 0.6035\n",
      "Epoch 20/20\n",
      "390/390 [==============================] - 107s 273ms/step - loss: 1.1475 - accuracy: 0.6040\n",
      "10000/10000 [==============================] - 9s 882us/step\n",
      "Test score: 1.0315151695251465\n",
      "Test accuracy: 0.6377000212669373\n"
     ]
    }
   ],
   "source": [
    "# Improving performance with data augmentation (generate more images)\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "NUM_TO_AUGMENT=5\n",
    "\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert to categorical\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# instantiate ImageDataGenerator to create approximately 10 images for\n",
    "# each input training image\n",
    "print(\"Augmenting training set images...\")\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "xtas, ytas = [], []\n",
    "for i in range(X_train.shape[0]):\n",
    "    num_aug = 0\n",
    "    x = X_train[i] # (3, 32, 32)\n",
    "    x = x.reshape((1,) + x.shape)  # (1, 3, 32, 32)\n",
    "\n",
    "    for x_aug in datagen.flow(x, batch_size=1,\n",
    "        save_to_dir='preview', save_prefix='cifar', save_format='jpeg'):\n",
    "        if num_aug >= NUM_TO_AUGMENT:\n",
    "            break\n",
    "        xtas.append(x_aug[0])\n",
    "        num_aug += 1\n",
    "\n",
    "#fit the dataget\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "# Added these lines for the deeper network\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "####\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# train\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit_generator(datagen.flow(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE), samples_per_epoch=X_train.shape[0],\n",
    "epochs=NB_EPOCH, verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test,\n",
    "batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Recognition Training\n",
    "Often times, the source of many potential ethical or privacy issues surrounding AI models stem from the methods used to train a model for a specific purpose. A significant collection of data is required for a system to realize a desired degree of functionality. When it comes to facial recognition technology, images of human faces from a wide variety of ethnicities or races are used to train a model on how to recognize particular patterns to identify a face. These images may be sourced from all over the web, like from social media platforms, where the individuals who uploaded such images may not have agreed for their data to be used to train a facial identification model. This presents a significant privacy issue that remains problematic for data collection in regards to AI training. \n",
    "## How the Data is Used\n",
    "What is more, the specific applications of facial identification systems using a trained AI model can present further ethical issues based on the context of how the system is used. According to Lunter of Biometric Technology Today, IBM chose to drop facial recognition software offerings by their organization due to the biases these systems may have when performing facial recognition. Cities within the United States, like San Francisco, have banned the technology for use within city government agencies, with police departments being of particular note. IBM and Amazon, who are the primary providers of this type of technology to police departments around the country, have refused to disclose the algorithms they have used to power these systems, which could be phrased as a failure to provide right to explanation. These systems have been shown to misidentify images of Black and Asian individuals, mostly due to the lack of training data used to represent these races, which could have serious negative implications in the hands of a police force, such as wrongfully arresting a suspect based on biased recognition data. \n",
    "The solution to at least the dilemma of correctly identifying differing races or ethnicities through facial recognition may lie primarily in providing adequate training data to the AI models that is both extensive and unbiased in terms of labelling and content. It is also worth mentioning that despite facial recognition technology containing a degree of bias towards particular races, it is mentioned that the analysis of police cases indicated humans were significantly worse at identifying the correct faces than even the poorest performing modern algorithms, with particular note to individuals being able to positively discern the faces of those that belong to their own race.\n",
    "## References\n",
    "Lunter J. (2020). Beating the bias in facial recognition technology. Biometric Technology Today, 2020(9), 5â€“7. https://doi.org/10.1016/S0969-4765(20)30122-3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
